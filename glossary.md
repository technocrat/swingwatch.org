+++
title = "Glossary"
+++


- **Agency detection bias**: Overdetecting agency and intentionality behind events, attributing events to intentional actions of agents, such as conspirators, rather than chance or natural causes. 

- **Anchoring bias**: Reyling too heavily on the first piece of information offered (the "anchor") when making decisions.

- **Attribution bias**: The tendency to attribute the behavior of others to their character or disposition, while attributing one's own behavior to situational factors.

- **Availability bias**: The tendency to overestimate the likelihood of events with greater "availability" in memory. Judging events that are more easily recalled as more frequent. Arises when participants do not remember past events accurately, skewing the data.

- **Bayesian statistics**: Bayesian statistics is a branch of statistics that applies the principles of probability theory to update the probability estimates for hypotheses as new evidence or data becomes available. It is based on Bayes' theorem, which describes the conditional probability of an event occurring given prior knowledge of the event's probability.

The key principles of Bayesian statistics are:

1. Prior probability: This is the initial probability assigned to a hypothesis before observing any new data. It represents the existing knowledge or belief about the hypothesis.

2. Likelihood: This is the probability of observing the data given the hypothesis is true. It measures how well the data supports the hypothesis.

3. Posterior probability: This is the updated probability of the hypothesis after considering the new data. It is calculated by combining the prior probability and the likelihood using Bayes' theorem.

The fundamental equation of Bayesian statistics is:

Posterior probability = (Prior probability × Likelihood) / Marginal likelihood

Unlike frequentist statistics, which relies on repeated sampling and long-run frequencies, Bayesian statistics treats probabilities as degrees of belief or plausibility and updates those beliefs as new data becomes available. Bayesian methods allow for the incorporation of prior knowledge or expert opinion into the analysis, which can be valuable when dealing with limited data or complex problems.

Bayesian statistics finds applications in various fields, including machine learning, data analysis, decision theory, and scientific research, where it provides a coherent framework for quantifying uncertainty and making inferences based on observed data and prior information.

- **Bounded rationality**: Bounded rationality is an an alternative to the classical economic theory of perfect rationality. When individuals make decisions, their rationality is limited by various factors, such as the available information, cognitive limitations, and time constraints. As a result, decision-makers often seek satisfactory ("satisficing") rather than optimal ("ideal") solutions. As proposed by Herbert Simon.

Key aspects of bounded rationality:

1. Limited information: Decision-makers often lack access to all relevant information and may not have the ability to process and interpret all available data.

2. Cognitive constraints: The human brain has limited computational capacity and is subject to various biases and heuristics that can lead to suboptimal decisions.

3. Time and resource constraints: Decision-makers often face time pressures and may not have the resources to thoroughly analyze all possible alternatives.

4. Satisficing: Instead of seeking the optimal solution, people often choose the first satisfactory option that meets their minimum requirements.

5. Heuristics: To cope with the limitations of rationality, individuals often rely on mental shortcuts or rules of thumb to simplify complex decisions.

6. Adaptive behavior: Bounded rationality acknowledges that people learn from their experiences and adapt their decision-making strategies over time.

Implications of bounded rationality:

1. Realistic decision-making models: By acknowledging the limitations of human rationality, bounded rationality helps develop more realistic models of decision-making in various fields, such as economics, psychology, and political science.

2. Organizational decision-making: Bounded rationality has important implications for understanding how organizations make decisions, as they are composed of individuals with limited rationality and are subject to various organizational constraints.

3. Choice architecture: Insights from bounded rationality have led to the development of "nudges" and other strategies that aim to improve decision-making by altering the context in which choices are made.

4. Satisficing vs. optimizing: Bounded rationality suggests that in many situations, satisficing (choosing a good enough option) may be a more practical and efficient approach than optimizing (seeking the best possible option).

Bounded rationality is a more realistic approach to understanding human decision-making that takes into account the various limitations and constraints that individuals face. 

- **Central limit theorem**: The Central Limit Theorem (CLT) is a fundamental concept in statistics that explains the behavior of the means of random samples drawn from a population. In simple terms, the theorem states that when you take many large random samples from any population, the distribution of the sample means will approximate a normal distribution, *regardless* of the shape of the original population distribution.

Key points of the Central Limit Theorem:

1. Sample size: The theorem applies when the sample size is sufficiently large (usually n ≥ 30).

2. Population distribution: The original population can have any distribution (normal, skewed, bimodal, etc.).

3. Sample means distribution: If you draw many random samples of the same size from the population and calculate the mean for each sample, the distribution of these sample means will be approximately normal.

4. Parameters of the sample means distribution:
   - The mean of the sample means will be equal to the mean of the population.
   - The standard deviation of the sample means (also called the standard error) will be equal to the population standard deviation divided by the square root of the sample size.

Implications of the Central Limit Theorem:

1. Statistical inference: CLT allows researchers to make inferences about the population mean based on the sample mean, even when the population distribution is unknown or non-normal.

2. Confidence intervals: The normal distribution of sample means enables the calculation of confidence intervals for the population mean.

3. Hypothesis testing: Many statistical tests, such as the t-test and ANOVA, rely on the assumption of normality, which is justified by the CLT when sample sizes are large enough.

In summary, the Central Limit Theorem is essential because it provides a foundation for many statistical methods and allows researchers to make inferences about populations based on samples, even when the population distribution is unknown or non-normal.

- **Cognitive bias**: A systematic pattern of deviation from norm or rationality in judgment, whereby inferences about other people and situations may be drawn in an illogical fashion. Individuals create their own "subjective reality" from their perception of the input. Cognitive biases may sometimes lead to perceptual distortion, inaccurate judgment, illogical interpretation, or what is broadly called irrationality. Cognitive biases are often a result of an individual's attempt to simplify information processing. They are rules of thumb that help us make sense of the world and reach decisions with relative speed. Some of these biases are related to memory. The way you remember an event may be biased for a number of reasons and that in turn can lead to biased thinking and decision-making. Other cognitive biases might be related to problems with attention, when information overload can cause people to focus on just a subset of the available information.

- **Confirmation bias**: The tendency to search for, interpret, favor, and recall information in a way that confirms or supports one's prior beliefs or values. 

- **Conjunction fallacy**: The tendency to think that specific conditions are more probable than more general ones. Judging the co-occurrence of two events as more likely than either event alone.

- **Convenience sampling**: A convenience sample is a type of non-probability sampling method where the sample is selected based on the convenience and accessibility of the participants. In other words, the researcher selects individuals who are easy to reach or readily available, rather than using a random selection process.

Characteristics of a convenience sample:

1. Non-random selection: Participants are chosen based on their proximity or availability to the researcher.

2. Ease of recruitment: Convenience samples are often less time-consuming and less expensive to gather compared to probability-based samples.

3. Limited generalizability: Because the sample is not randomly selected, the results may not be representative of the entire population. This limits the ability to make statistical inferences or generalize the findings to the larger population.

4. Potential for bias: Convenience samples may be more prone to selection bias, as the sample may not accurately reflect the characteristics of the population. This can lead to over- or under-representation of certain groups.

Examples of convenience sampling:

1. A researcher surveying college students by sampling from their own classes.

2. An online survey shared on social media platforms, where participants self-select to respond.

3. A street survey where a researcher interviews people in a busy shopping area.

Despite its limitations, convenience sampling can be useful in certain situations, such as:

1. Pilot studies: Convenience samples can help researchers test and refine their methods before conducting a larger, more rigorous study.

2. Hard-to-reach populations: When studying populations that are difficult to access or identify, convenience sampling may be the only feasible option.

3. Exploratory research: Convenience samples can provide initial insights and generate hypotheses for further research.

However, it is essential for researchers to recognize and acknowledge the limitations of convenience sampling and to be cautious when interpreting and generalizing the results. 

- **Economic rationality**: Economic rationality is a fundamental concept in economics that describes the decision-making process of individuals or entities, assuming they are motivated by self-interest and aim to maximize their utility or benefit while minimizing costs. In other words, economic rationality posits that people make decisions based on logical reasoning and the available information to achieve the best possible outcome for themselves.

Key assumptions of economic rationality:
1. Consistency: Individuals have consistent preferences and make decisions that align with those preferences.
2. Perfect information: Decision-makers have access to all relevant information and can process it effectively.
3. Self-interest: People make choices that maximize their own utility or well-being.
4. Optimization: Individuals seek the best possible outcome given the constraints they face.

Criticisms of economic rationality:
1. Bounded rationality: Herbert Simon argued that humans have cognitive limitations that prevent them from always making optimal decisions. Instead, they often use heuristics or rules of thumb to make satisfactory, rather than optimal, choices.

2. Psychological factors: Behavioral economists like Daniel Kahneman and Amos Tversky demonstrated that psychological factors, such as emotions, biases, and heuristics, significantly influence decision-making, leading to deviations from perfect rationality.

3. Social and cultural influences: Critics argue that economic rationality neglects the role of social norms, cultural values, and interpersonal relationships in shaping individual preferences and decisions.

4. Altruism and cooperation: Economic rationality struggles to explain altruistic behavior or cooperation, as these actions may not always maximize an individual's self-interest in the short term.

5. Incomplete information: In reality, decision-makers rarely have access to perfect information, leading to suboptimal choices.

6. Adaptive preferences: People's preferences may change over time or adapt to their circumstances, challenging the assumption of consistent preferences.

7. Ethical considerations: Some argue that focusing solely on self-interest and utility maximization may lead to unethical or socially harmful decisions.

Despite these criticisms, economic rationality remains a useful model for understanding and predicting economic behavior in many situations. However, economists increasingly recognize the importance of incorporating insights from behavioral economics and other disciplines to develop more realistic and comprehensive models of decision-making.

- **Expectation bias**: The inclination to perceive what we expect to perceive.

- **Framing bias**: Arises when participants do not remember past events accurately, skewing the data.

- **Funding bias**:  The conflict of interest problem of funding influencing the research process or outcomes to favor the sponsor's interests.

- **Ground truth**: A statement regarding something indirectly observed. In presidential preference polling, ground truth can never be checked because it is not practical to obtain survey answers from all potential voters. The election is held in the future, is not directly observable in advance, and is not repeatable.

- **Groupthink**: The tendency for members of a group to conform to the prevailing views and suppress dissenting opinions to maintain group harmony.

- **heuristic**: A heuristic is a mental shortcut or a rule of thumb that allows people to make judgments and decisions quickly and efficiently. Heuristics are often used when faced with complex problems or incomplete information, as they can help simplify the decision-making process by reducing the amount of mental effort required. While heuristics can be useful in many situations, they can also lead to cognitive biases and systematic errors in judgment.

Key characteristics of heuristics:

1. Efficiency: Heuristics enable individuals to make decisions quickly by focusing on the most relevant information and ignoring less essential details.

2. Cognitive economy: By using heuristics, people conserve mental resources and avoid the need for extensive information processing.

3. Implicit learning: Heuristics are often developed through experience and observation, rather than explicit instruction.

4. Adaptiveness: Heuristics can be adaptive, helping individuals navigate complex environments and make decisions under uncertainty.

5. Potential for bias: While heuristics are often useful, they can also lead to systematic errors and biases in judgment when applied inappropriately or when the simplified rule does not fit the situation.

Examples of common heuristics:

1. Availability heuristic: Judging the likelihood of an event based on how easily examples come to mind.

2. Representativeness heuristic: Making judgments based on how similar an object or event is to a typical case or stereotype.

3. Anchoring and adjustment: Relying heavily on the first piece of information encountered (the anchor) and making insufficient adjustments based on additional information.

4. Affect heuristic: Basing decisions on emotional responses rather than objective evaluations.

5. Recognition heuristic: Choosing the option that is most familiar or recognizable.

While heuristics are an essential part of human cognition and can be helpful in many situations, it is important to be aware of their potential limitations and biases. By understanding when and how heuristics are used, individuals can make more informed decisions and avoid common pitfalls in judgment. Additionally, researchers in fields such as psychology, economics, and decision science study heuristics to better understand human behavior and develop strategies for improving decision-making processes.

- **Hindsight bias**: The inclination to see past events as more predictable than they actually were, often leading to an overestimation of one's ability to predict future events.

- **Illusory correlation**: The tendency to perceive a relationship between variables even when no such relationship exists.

- **IVR**: Interactive Voice Response (IVR) is a technology that allows a computer to interact with humans through the use of voice and DTMF (Dual-Tone Multi-Frequency) tones input via a keypad. In the context of surveys, IVR enables researchers to conduct automated telephone surveys without the need for human interviewers.

Here's how an IVR survey typically works:

1. The IVR system dials a list of telephone numbers.

2. When a call is answered, the system plays a pre-recorded or dynamically generated voice message that introduces the survey and provides instructions.

3. The respondent is then prompted to answer a series of questions by either speaking their responses or entering them using the phone's keypad.

4. The IVR system records the responses and moves on to the next question based on the answers provided and the survey's logic.

5. Once the survey is completed, the system thanks the respondent and ends the call.

Some key features and benefits of IVR surveys include:

1. Automation: IVR surveys can be conducted without human interviewers, reducing costs and allowing for larger sample sizes.

2. Consistency: All respondents hear the same questions in the same order, reducing variability due to interviewer bias.

3. 24/7 availability: IVR surveys can be conducted at any time, allowing respondents to complete the survey at their convenience.

4. Multilingual support: IVR systems can offer surveys in multiple languages, making them more accessible to diverse populations.

However, IVR surveys also have some limitations:

1. Limited complexity: IVR surveys are best suited for short, simple questions with clear answer choices.

2. Lower response rates: Some people may be less likely to complete an automated survey than one conducted by a human interviewer.

3. Lack of personal touch: The automated nature of IVR surveys can make them feel impersonal, potentially affecting response quality.

Despite these limitations, IVR surveys remain a valuable tool for researchers, particularly when conducting large-scale, cost-effective surveys.

- **Jumping-to-conclusions bias (JTC)**: The tendency to make decisions based on limited information before gathering sufficient evidence. Making hasty judgments prematurely.

- **Mail-in polls**: A type of opt-in survey.

- **Opt-in survey**: A survey that requires explicit prior consent. They can lead to skewed results, such as overestimating fringe beliefs or the percentage of people holding certain qualifications, especially among younger respondents. The quality of data collected through these surveys is questionable, as participants may speed through surveys to earn money, leading to inaccurate responses. Researchers use online opt-in surveys because they are cheaper, faster, and more convenient than probability-based surveys, despite potential data quality issues. A study found that a significant portion of respondents in an opt-in survey may have misrepresented their background to qualify for the survey and earn money. Researchers can minimize bogus responses by using attention checks, internet protocol address tracking, anti-bot software, and monitoring survey completion time. However, some low-quality responses may still slip through. Alternative models, such as opt-in volunteer surveys, can create different incentives for participants that do not  rely on financial rewards.

- **Mirror imaging**: The assumption that others (such as political candidates) will act in a way consistent with one's own thought processes and values.

- **Normal distribution**: A variable is said to be normally distributed if it follows a normal distribution, also known as a Gaussian distribution or a "bell curve." The normal distribution is a continuous probability distribution that is symmetric about its mean, with data points clustered around the mean and tapering off symmetrically as the distance from the mean increases. Many natural phenomena and variables, such as heights, weights, and test scores, often follow a normal distribution.

Key characteristics of a normal distribution:

1. Symmetry: The normal distribution is symmetric around its mean, with an equal number of data points on both sides of the mean.

2. Bell-shaped curve: When plotted, the normal distribution forms a bell-shaped curve, with the highest point at the mean and the curve tapering off symmetrically on both sides.

3. Mean, median, and mode: In a normal distribution, the mean, median, and mode are all equal and located at the center of the distribution.

4. Defined by mean and standard deviation: A normal distribution is fully described by its mean (μ) and standard deviation (σ). The mean determines the location of the center of the distribution, while the standard deviation measures the spread of the data points.

5. 68-95-99.7 rule: In a normal distribution, approximately 68% of the data points fall within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations.

6. Infinite range: Theoretically, the normal distribution extends infinitely in both directions, although most of the data points are concentrated near the mean.

7. Central Limit Theorem: The normal distribution is crucial in statistics due to the Central Limit Theorem, which states that the sampling distribution of the mean of any independent, random variable will be normal or nearly normal, if the sample size is large enough.

Properties of the standard normal distribution:

1. Standard normal distribution: A special case of the normal distribution with a mean of 0 and a standard deviation of 1.

2. Z-scores: In a standard normal distribution, data points are often expressed as z-scores, which measure the number of standard deviations a data point is from the mean.

3. Probability calculations: The standard normal distribution allows for easy probability calculations using z-scores and standard normal tables or statistical software.

Understanding normal distributions is essential in statistics, as many inferential techniques, such as t-tests, ANOVA, and regression analysis, assume that the variables involved are normally distributed. When data is not normally distributed, researchers may need to use alternative methods or transform the data to meet the normality assumption.

- **Observer bias**: Results from researchers' subjective expectations influencing their observations or interpretations.

- **Omitted variable bias**: Happens when important variables are left out of an analysis, leading to incorrect relationships being identified.

- **Overconfidence**: Having excessive confidence in one's own answers to questions. Overestimating one's own skills, abilities and accuracy of beliefs. 

- **Parameter**: A statistical parameter is a numerical summary measure that describes a characteristic of a population. Unlike a statistic, which is a summary measure calculated from a sample, a parameter represents a fixed value that is typically unknown and is estimated using sample data. Parameters are often denoted using Greek letters, such as μ (mu) for the population mean and σ (sigma) for the population standard deviation.

Key points about statistical parameters:

1. Population characteristic: A parameter describes a specific attribute of an entire population, such as its center, spread, or shape.

2. Fixed value: A parameter is a fixed, unknown value that does not change unless the population itself changes.

3. Estimation: Since measuring an entire population is often impractical or impossible, parameters are typically estimated using sample data and inferential statistics.

4. Inference: The goal of many statistical analyses is to use sample data to make inferences about population parameters.

5. Symbols: Parameters are usually represented by Greek letters to distinguish them from sample statistics, which are typically denoted by Latin letters (e.g., x̄ for sample mean, s for sample standard deviation).

Examples of common statistical parameters:

1. Population mean (μ): The arithmetic average of all values in a population.

2. Population variance (σ²): The average of the squared deviations from the population mean.

3. Population standard deviation (σ): The square root of the population variance, measuring the spread of values around the mean.

4. Population proportion (p): The fraction or percentage of the population that possesses a specific characteristic.

5. Population correlation coefficient (ρ): A measure of the linear relationship between two variables in a population.

In practice, researchers use sample data to calculate statistics, which are then used to estimate the corresponding population parameters. For example, a sample mean (x̄) can be used to estimate the population mean (μ). Inferential statistics, such as confidence intervals and hypothesis tests, help quantify the uncertainty associated with these estimates and allow researchers to make informed conclusions about population parameters based on sample data.

- **Poll aggregation**: Combining the results of multiple polls, also known as "poll aggregation," is a common practice to improve the accuracy and reliability of the overall estimate. There are several methods to do this, and the choice depends on factors such as the number of polls, their sample sizes, and the consistency of their results. Here are a few common approaches:

1. Simple average: Calculate the average percentage for each candidate across all polls. This method treats all polls equally, regardless of their sample size or quality.

2. Weighted average: Assign weights to each poll based on factors such as sample size, recency, or pollster rating, and then calculate the weighted average for each candidate. This method gives more importance to polls considered more reliable or representative. This is the method used to report aggregated swing state polls on the [home page](../index).

3. Inverse variance weighting: Weight each poll by the inverse of its variance (which is related to the margin of error). This method gives more weight to polls with smaller margins of error, as they are considered more precise. This is not currently being done because margins of error are similar.

4. Bayesian aggregation: Use a Bayesian model to estimate the probability distribution of the true proportion of support for each candidate, considering the results and uncertainty of each poll. This method can incorporate prior information and account for various sources of uncertainty. This is being done to arrive at an overall likelihood based on currrent polling in light of the 2020 results.

- **Population**: A collection of units of observation, such as registered voters, likely voters, likely voters who identify as white, male conservative registered Democrats, taken as a whole. Given knowledge of all such units, it is desired to classify them. When they cannot all be identified and queried, a sample is relied upon to be representative.

- **Probability based sample**: A probability-based sample is a type of sample in which every unit in the population has a known, non-zero probability of being selected. This sampling method relies on the principles of probability theory to choose a sample that is representative of the population of interest. The main advantage of probability-based sampling is that it allows researchers to make statistical inferences about the population based on the sample results.

Key features of a probability-based sample:

1. Random selection: Each unit in the population has an equal or known chance of being selected.

2. Representativeness: The sample is intended to be representative of the population, meaning that the characteristics of the sample closely resemble those of the population.

3. Generalizability: Because the sample is representative, findings from the study can be generalized to the larger population, within a certain margin of error.

4. Reduced bias: Probability-based sampling helps minimize selection bias, as the sample is chosen based on chance rather than the researcher's preferences or convenience.

Common probability-based sampling methods include:

1. Simple random sampling: Each unit in the population has an equal chance of being selected.

2. Stratified random sampling: The population is divided into subgroups (strata) based on specific characteristics, and then units are randomly selected from each stratum.

3. Cluster sampling: The population is divided into clusters (e.g., geographic areas), and then a random sample of clusters is selected. All units within the selected clusters are included in the sample.

4. Systematic sampling: Units are selected from the population at regular intervals (e.g., every 10th unit on a list) after a random starting point.

In contrast, non-probability sampling methods, such as convenience sampling or snowball sampling, do not give every unit in the population a known chance of being selected. While these methods can be useful in certain situations, they do not allow for statistical inference about the population and may be more prone to bias.

- **Proportionality bias**: The belief that causes should resemble effects in size or magnitude. Large effects are assumed to have large causes.

- **Random sample**: A random sample is a subset of individuals chosen from a larger population in such a way that each individual has an equal probability of being selected. The purpose of taking a random sample is to obtain a representative group that can be used to make inferences about the larger population without bias.

Key characteristics of a random sample:

1. Equal probability of selection: Each member of the population has the same chance of being included in the sample.

2. Independence: The selection of one individual does not affect the probability of selecting any other individual.

3. Representativeness: A well-chosen random sample should be representative of the population, meaning that the characteristics of the sample closely mirror those of the larger population.

4. Unbiased: Random sampling helps minimize bias in the selection process, as it eliminates the possibility of the researcher consciously or unconsciously choosing individuals based on specific characteristics.

IID (Independent and Identically Distributed):
In the context of random sampling, IID is a crucial concept. It means that each observation or data point in the sample is independent of the others and comes from the same underlying probability distribution.

1. Independence: The value of one observation does not influence the value of another observation. In other words, knowing the value of one data point provides no information about the value of any other data point.

2. Identical distribution: All observations in the sample come from the same probability distribution. This means that the sample data's statistical properties, such as mean and variance, are consistent across the sample and representative of the population.

When a random sample is IID, it allows researchers to use various statistical methods to make inferences about the population. For example, the Central Limit Theorem, which states that the distribution of sample means approaches a normal distribution as the sample size increases, relies on the assumption that the sample is IID.

However, in practice, achieving a truly IID random sample can be challenging. Factors such as sampling bias, non-response bias, and measurement errors can introduce dependence or differences in the distribution of observations. Researchers must be aware of these potential issues and take steps to minimize their impact on the sample's representativeness.

- **Recall bias**: Arises when participants do not remember past events accurately, skewing the data.

- **Representativeness heuristic**: Judging the probability of an event by finding a 'comparable known' event and assuming that the probabilities will be similar. Assuming that two things that share characteristics are related.

- **Resistance to change**: Perceptions resist change even when new evidence is presented.

- **Sample space**: In probability theory, a sample space is the set of all possible outcomes of a random experiment or a probability experiment. It is usually denoted by the symbol "S" or "Ω" (omega).

Key points about sample space:

1. **Exhaustive** The sample space must include all possible outcomes of the experiment. No outcome should be left out.

2. **Mutually exclusive**: The outcomes in the sample space should be distinct and non-overlapping. The occurrence of one outcome means that no other outcome has occurred.

- **Selection bias**: The sample is not representative of the population due to the method of selection. Selection bias is a type of bias that occurs when the sample chosen for a study is not representative of the population intended to be analyzed. This can happen when the selection process favors some individuals or groups over others, leading to a sample that systematically differs from the target population. As a result, the findings based on the biased sample may not be generalizable to the entire population.

Types and causes of selection bias:

1. Sampling bias: This occurs when the method used to select the sample is not truly random, leading to over- or under-representation of certain groups. For example, a telephone survey that only calls landlines may underrepresent younger individuals who primarily use mobile phones.

2. Self-selection bias: This happens when individuals voluntarily participate in a study, and those who choose to participate differ from those who do not. For instance, people with strong opinions on a topic may be more likely to respond to a survey about that topic, leading to a biased sample.

3. Attrition bias: This occurs when participants drop out of a study non-randomly, causing the remaining sample to differ from the original sample. For example, in a longitudinal study on the effects of a drug, individuals who experience severe side effects may be more likely to drop out, leading to an underestimation of the drug's negative impacts.

4. Exclusion bias: This happens when certain individuals or groups are systematically excluded from the sample, often due to the study's design or eligibility criteria. For instance, a study on the effects of a medication that excludes pregnant women may not be generalizable to that population.

5. Healthy user bias: This occurs when individuals who engage in healthier behaviors are more likely to participate in a study or adhere to a treatment, making the treatment appear more effective than it is.

Consequences of selection bias:

1. Reduced generalizability: Findings based on a biased sample may not be applicable to the entire population, limiting the study's external validity.

2. Inaccurate conclusions: Selection bias can lead to overestimating or underestimating the true effect of an intervention or the prevalence of a condition in the population.

3. Flawed decision-making: Policies or decisions based on biased research may be ineffective or even harmful.

To minimize selection bias, researchers should:

1. Use truly random sampling methods when possible.
2. Clearly define the target population and eligibility criteria.
3. Monitor and address issues of non-response and attrition.
4. Use statistical methods to adjust for known biases in the sample.
5. Be transparent about the study's limitations and potential sources of bias.

By understanding and addressing selection bias, researchers can improve the quality and reliability of their findings, leading to more accurate conclusions and better-informed decision-making.

- **Self-serving bias**: The common habit of a person taking credit for positive events or outcomes, but blaming outside factors for negative events.

- **Simple random sampling error**: Also known as sampling error, it is the difference between a sample statistic and the corresponding population parameter due to the inherent variability that arises from randomly selecting a subset of the population. This error occurs because a sample, even when randomly selected, may not perfectly represent the entire population,**even a population that is normally distributed**, leading to estimates that differ from the true population values.

Key points about simple random sampling error:

1. Inherent variability: Sampling error is an unavoidable consequence of using a sample to estimate population parameters, as no sample can perfectly mirror the population.

2. Random sampling: Simple random sampling error occurs when a sample is selected using a random process, ensuring that each member of the population has an equal chance of being included in the sample.

3. Sample size: Generally, larger sample sizes tend to have smaller sampling errors, as they are more likely to be representative of the population.

4. Variability: Sampling error is a measure of the variability between different samples drawn from the same population.

5. Estimation: Sampling error affects the accuracy of estimates made about population parameters based on sample statistics.

6. Confidence intervals: Sampling error is often quantified using confidence intervals, which provide a range of plausible values for the population parameter based on the sample data and the desired level of confidence.

7. Margin of error: The margin of error is a common way to express sampling error, representing the maximum expected difference between the sample statistic and the population parameter at a given confidence level.

Factors affecting simple random sampling error:

1. Sample size: Increasing the sample size generally reduces sampling error, as larger samples are more likely to be representative of the population.

2. Population variability: More heterogeneous populations tend to have larger sampling errors, as a sample is less likely to capture all the variability in the population.

3. Sampling design: While simple random sampling is designed to minimize bias, other sampling methods (e.g., stratified sampling) may be used to reduce sampling error by ensuring that important subgroups are adequately represented in the sample.

It is important to note that sampling error is distinct from other sources of error in a study, such as measurement error or non-response bias. While sampling error is an inherent part of using samples to make inferences about populations, researchers can minimize its impact by using appropriate sampling methods, increasing sample sizes when feasible, and reporting the uncertainty associated with their estimates using confidence intervals or margins of error.
- **Statistical bias**: A systematic error that results in a difference between the true value (or ground truth) of a statistical parameter and its estimated value obtained from a sample. This can arise from sources of data collection, analysis, or interpretation, leading to results that are not representative of the entire population. Bias affects the validity and reliability of conclusions drawn from a statistical method. In the analysis of presidential preference poll results, some measure can be captured by an allowance for **design error** that affects the data collection process.

- **Survivorship bias**: Involves focusing on surviving or existing data while overlooking data that does not make it past some point of the process.

- **System 1 thinking**: Fast, automatic, emotional and subconscious style of thinking and judgment. Relies on heuristics and produces systematic errors.

- **System 2 thinking**: Slow, effortful, controlled and conscious style of reasoning and analysis. Logical and rule-based. Can overcome biases from System 1.

- **Vividness criterion**: The tendency to give more weight to concrete, emotionally compelling information than to abstract or statistical information.